% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ll_regression_forest2.R
\name{predict.ll_regression_forest2}
\alias{predict.ll_regression_forest2}
\title{Predict with a local linear forest}
\usage{
\method{predict}{ll_regression_forest2}(
  object,
  newdata = NULL,
  linear.correction.variables = NULL,
  ll.elnet.alpha = NULL,
  ll.lambda = NULL,
  ll.weight.penalty = FALSE,
  num.threads = NULL,
  estimate.variance = FALSE,
  thresh = NULL,
  maxit = NULL,
  ...
)
}
\arguments{
\item{object}{The trained forest.}

\item{newdata}{Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.}

\item{linear.correction.variables}{Optional subset of indexes for variables to be used in local
linear prediction. If left NULL, all variables are used.
We run a locally weighted linear regression on the included variables.
Please note that this is a beta feature still in development, and may slow down
prediction considerably. Defaults to NULL.}

\item{ll.elnet.alpha}{The elastic net mixing parameter, a proportion bound within 0 and 1. The parameter \code{ll.elnet.alpha}
is defined as \eqn{\alpha} in the elastic net penalty parameterization
\deqn{(1-\alpha)/2||\beta||_2^2+\alpha||\beta||_1.} \code{alpha=1} is the lasso penalty,
and \code{alpha=0} the ridge penalty. By default, uses the value of \code{ll.elnet.alpha} used
for training the forest.}

\item{ll.lambda}{Ridge penalty for local linear predictions. Defaults to NULL and will be cross-validated.}

\item{ll.weight.penalty}{Option to standardize ridge penalty by covariance (TRUE),
or penalize all covariates equally (FALSE). Defaults to FALSE.}

\item{num.threads}{Number of threads used in training. If set to NULL, the software
automatically selects an appropriate amount.}

\item{estimate.variance}{Whether variance estimates for hat{tau}(x) are desired
(for confidence intervals).}

\item{thresh}{Convergence threshold for coordinate descent. Each coordinate descent loop continues until the
maximum change in the objective after any coefficient update is less than \code{thresh} times
the null deviance. By default, uses the same value of \code{thresh} used for training the forest.}

\item{maxit}{Maximum number of passes over the data for all lambda values. By default, uses the same value of
\code{maxit} used for training the forest.}

\item{...}{Additional arguments (currently ignored).}
}
\value{
A vector of predictions.
}
\description{
Gets estimates of E[Y|X=x] using a trained regression forest.
}
\examples{
\donttest{
# Train the forest.
n <- 50
p <- 5
X <- matrix(rnorm(n * p), n, p)
Y <- X[, 1] * rnorm(n)
forest <- ll_regression_forest(X, Y)

# Predict using the forest.
X.test <- matrix(0, 101, p)
X.test[, 1] <- seq(-2, 2, length.out = 101)
predictions <- predict(forest, X.test)

# Predict on out-of-bag training samples.
predictions.oob <- predict(forest)
}

}
